```{r global_options, include=FALSE}
# Define libraries.
library(knitr)
library(reshape2)
library(ggplot2)
library(doParallel)
library(pROC)

# Register the number of cores.
registerDoParallel(cores=detectCores())

opts_chunk$set(fig.width=6, fig.height=4, fig.align='center', fig.path='imgs/',
               message=FALSE, warning=FALSE)
```
---
title: "Practical Machine Learning Assignment"
author: "Duncan Paul Attard"
date: "23, January 2015"
output: 
  html_document:
---

# Introduction

This report describes the analysis and design required to implement the set assignment for the [Practical Machine Learning](https://class.coursera.org/predmachlearn-010) course. It first outlines the basic task assigned, and then goes on to explaining how the data was analyzed, the relevant features selected, and also how the model was built. Finally, the performance of the model is measured and evaluated. The required results for the 20 test examples are listed thereafter.

## Background

> "Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)." ^1^

^1^ Copied verbatim out of the assigned problem description.

The aim was to utilize the data mentioned above to train a classification model with which new and unseen test examples are predicted.

## Research

Manual examination of the training and test data revealed that the measurements taken are of the following types:

* Accelerometer data: These measure inertial acceleration.
* Gyroscope data: These measure changes in orientation with reference to a starting position.
* Magnetometer data: These measure magnetic fields.

 More generally, these three measurements combined together detect acceleration, and also changes in rotational attributes, namely the roll, pitch and yaw. In addition, it is common to find these three components housed into a single electronic device known as an IMU (Inertial Measurement Unit). For more information on the topic, visit the Wikipedia [link](http://en.wikipedia.org/wiki/Inertial_measurement_unit) from where this information was adapted.

# Pre-processing

This section explains how the data was loaded and cleaned, and from this, how the training features were selected. 

## Loading and Cleaning the Data

The data used for completing this assignment can be found at this [link](http://groupware.les.inf.puc-rio.br/har). The [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and [evaluation](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) data sets are provided in the form of CSV web resources. The training data was used to train the data model, whereas the evaluation data was used to evaluate the built model (this eveluation yielded the answers required for the programming part of this assigment). Through this, we were able to gauge the overall model performance. **Note** that even though the test data file is named `pml-testing.csv`, we refer to this file as having the evaluation data and **not** the test data. This is to avoid naming confusion with the test set that is generated by partitioning the data using `createDataPartition()`.

Once the data sets were downloaded on the local machine and examined, it was immediately noticed that much of the observation features contained empty or invalid values and these had to be filtered out first. To this end, the `readCleanData()` function was created. It loads data from a given file path, and removes any features having observations containing a large number of empty spaces, `NA` or `#DIV/0!` values. This, it does by first loading the file and transforming these values to `NA`s. Then, the ratio of `NA` values for each feature over *all* observations is calculated. Features are discared based on a simple threshold value, which in our case, was set to be `0.1`. This means that for a feature to be valid, it must contain *at least* 10% of non-NA values over *all* observations for that feature. More information can be obtained by examining the [readCleanData.R](https://github.com/duncanatt/pml-assignment/blob/master/readCleanData.R) file. Both the training and evaluation data sets are read in using this method, as shown underneath:

```{r clean_data}
# Define libraries.
library(caret)
library(ggplot2)

# Load helper functions.
source("readCleanData.R")
source("filterFeaturesByName.R")

# Load training and evaluation data.
cleanData <- readCleanData("pml-training.csv", na.thres = 0.1)
dim(cleanData)
cleanEval <- readCleanData("pml-testing.csv" , na.thres = 0.1)
dim(cleanEval)
```

After the cleaning process was completed, `r ncol(cleanData)` features remained; all the others were discarded. From this clean data, the features with which the data model was built were selected next.

## Feature Selection

The second stage of pre-processing required the cleaned data to be analyzed, so that a sensible subset of features is extracted. Ideally, this new set contained only those features which are highly relevant to the classification task at hand. This meant that these features capture a lot variability, as this helps us distinguish between different observations more easily. Near-zero values could also be weeded out, as these do not provide usable data to our model. 

```{r filter_features}
# Define the list of non-required features.
nonReqFeatures <- c("X", 
                    "user_name", 
                    "raw_timestamp_part_1", 
                    "raw_timestamp_part_2", 
                    "cvtd_timestamp", 
                    "new_window", 
                    "num_window", 
                    "total_accel_belt", 
                    "total_accel_arm", 
                    "total_accel_dumbbell", 
                    "total_accel_forearm")

# Remove non-required features.
data <- filterFeaturesByName(cleanData, nonReqFeatures)
dim(data)
eval <- filterFeaturesByName(cleanEval, nonReqFeatures)
dim(eval)
```

After some background research as well as observation of the data, the `r length(nonReqFeatures)` features listed in the table below were chosen to be removed, so that only `r ncol(data) - 1` features were retained. The classification variable `classe` was also kept in the training set for convenience, since this was used in the formula passed to the `train()` function. Features were retained based on their variability and non-near-zero value, as explained above. Also, manual examination and insight into what each feature represents also proved useful when removing features. For example `user_name` is not directly related to the training outcome value of `classe`. Timestamp-related features also did not appear to be related with the training outcome. Features were removed using the `filterFeaturesByName()` function, the source of which can be viewed [here](https://github.com/duncanatt/pml-assignment/blob/master/filterFeaturesByName.R).

The remaining features could have been also checked for correlations, in an effort to reduce the feature set futher. Only one single feature was found to be correlated using a cutoff setting of 0.99, namely `accel_belt_z`. Manual inspection revealed that this single feature could be very well left in the original feature set.

A final close inspection of the filtered data revealed that no `NA` values were present in any feature (using `sum(is.na(cleanData)) == 0`), and as a consquence, no imputation was required from our end.

Index in Data Set | Removed Feature Name
----------------- | --------------------
1                 | X                    
2                 | user_name            
3                 | raw_timestamp_part_1 
4                 | raw_timestamp_part_2 
5                 | cvtd_timestamp       
6                 | new_window           
7                 | num_window           
11                | total_accel_belt     
49                | total_accel_arm      
102               | total_accel_dumbbell 
140               | total_accel_forearm  

The chosen features (`r ncol(data) - 1`) are shown in the figure below, as histogram plots.

```{r plot_feature_histograms, echo = FALSE, fig.width=8, fig.height=24}
# Plot a panel of histograms, one for each feature.
meltHist <- melt(data[, -49], id.vars = NULL, value.name = "Value", variable.name = "Feature")
ggplot(data = meltHist, aes(x = Value)) + 
    facet_wrap(~Feature, scales = "free_x", ncol = 4) + 
    geom_histogram() +
    xlab("Feature Value") + ylab("Observation Frequency") + ggtitle("Selected Features vs. Frequency")
```

# Implementation

The data set with reduced features was randomly partitioned into training and test sets, on `classe` using a 70%-30% split for the training and test sets respectively, as shown below:

```{r partition_data}
# Partition cleaned and filtered data into the training and test sets.
set.seed(1983)
parts <- createDataPartition(data$classe, p=0.7, list=FALSE)
train <- data[parts, ]
test <- data[-parts, ]
```

The historgams above reveal that not all the data is normally distributed, nor is it scaled. It turns out however that shifting and scaling is not required in our case, since random forests were used to build the data model. Random forests do not require the data to be normalized, as opposed to other gradient descent-based methods such as neural nets, linear regression or SVMs. 

## Training the Model

The training data was fit using a random forest model. Caret defaults to using bootstrap resampling. Yet, for this particular model, 5-fold cross-validation resampling was used.

```{r start_timer, echo = FALSE}
# Keep track of the start time.
start = Sys.time()
```

```{r fit_model}
# Fit first model using the 10-fold cross-validation resampling.
modFitRF <- train(classe ~ ., data = train, method = "rf", trControl = trainControl(method = "cv",number = 5), prox = TRUE, importance = TRUE)
modFitRF
modFitRF$finalModel
```

## Testing Predictions

Once the model was trained, this was validated against the test set we held out earlier on. The actual out-of-sample error was retrieved from the confusion matrix of the model.

```{r predict_values}
# Predict test classes using random forest model.
predRF <- predict(modFitRF, test[, -49])
cmRF <- confusionMatrix(predRF, test$classe)
cmRF
```

```{r calculate_error_estimates, echo = FALSE}
# Calculate the class range to select from the finalModel confustion matrix.
classRange <- seq_along(modFitRF$finalModel$classes)

# Calculate the estimated out-of-sample (or OOB) error from the finalModel confustion matrix as a percentage.
estimatedErr <- round((1 - (sum(diag(modFitRF$finalModel$confusion)) / sum(modFitRF$finalModel$confusion[classRange, classRange]))) * 100, 2)

# Calculate the actual out-of-sample error from the confusion matrix taken against the test set as a percentage.
actualErr <- round((1 - cmRF$overall[1]) * 100, 2)

# Finally calculate the total running time in seconds.
totTime <- as.integer(Sys.time() - start)
``` 

# Results

For the random forest model, the esitmated out-of-sample error obtained from the training set confusion matrix (i.e. `modFitRF$finalModel$confusion`) was `r estimatedErr`%, while the *actual* out-of-sample error obtained from the confusion matrix on the predicted test set was `r actualErr`%. This means a `r abs(estimatedErr - actualErr)`% difference between the estimated and actual out-of-sample error. The whole model took `r totTime` minutes to train using a parellalized setup with `r detectCores()` cores.

```{r calculate_small_change_in_error_elbow, echo = FALSE}
# Calculate the difference between each error position in OOB for each generated tree.
# This we will use to try to get an estimate of where an 'elbow' in the error
# plot is found. This will allow us to identify the point in the error plot where
# the error rate was decreasing very slowly, compared to the number of trees.
smallDeltaThres = 0.0005
firstErr <- c(modFitRF$finalModel$err.rate[, 1], 0)
secondErr <- c(0, modFitRF$finalModel$err.rate[, 1])

# Find the absolute error between the OOB of each tree.
absErrDiff <- abs(firstErr - secondErr)[-length(firstErr)]

# Find the index of the tree at which the error changes were very small. This 
# value will be used to plot a line on the error figure.
elbow <- (length(firstErr) - 1) - match(FALSE, rev(absErrDiff < smallDeltaThres))
```

The figure below shows a plot of the number of trees in the forest against the OOB error for each class. It also shows a vertical line (plotted at xintercept = `r elbow`) where the rate of change of OOB error between subsequent trees grown in the forest suddenly stopped decreasing rapidly (shown by the plot 'elbow bend'). This meant that the classification accuracy from that point onwards was only slightly increasing as the number of trees in the forest grew.

```{r plot_trees_vs_error, echo = FALSE}
# Plot the number of trees forest against the error for each class.
meltErr <- melt(modFitRF$finalModel$err.rate[, classRange + 1], value.name = "Error", varnames = c("Trees", "Classe"))
ggplot(data = meltErr, aes(x = Trees, y = Error, group = Classe, colour = Classe)) +
     geom_line() +
     ggtitle("Number of Trees vs. Error") + 
     geom_vline(xintercept = elbow)
```

The confusion matrix generated for the test set (also shown above in textual form) is plotted as a normalized heat map. It illustrates the actual class (ground truths) on the horizonal axis, and the predicted class on the vertical axis. 

```{r plot_test_confusion_matrix, echo = FALSE}
# Normalize the confusion matrix used with the test set, and convert it into a data frame.
cmNorm <- as.data.frame(as.table(apply(cmRF$table, 1, function(x)(x - mean(x)) / sd(x))))

# Set the appripriate columns to the data frame.
colnames(cmNorm) <- c("Prediction", "Reference", "Freq")

# Plot the confusion matrix heat map.
ggplot(cmNorm, aes(x = Reference, y = Prediction)) +
    geom_tile(aes(fill = Freq)) + 
    geom_text(aes(fill = cmNorm$Freq, label = round(cmNorm$Freq, 4)), size = 4) +
    scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) +
    scale_x_discrete(name = "Actual Class") + 
    scale_y_discrete(name = "Predicted Class") + 
    labs(fill = "Normalized\nFrequency") +
    ggtitle("Normalized Confusion Matrix on Test Set")
```

## Evaluation

The final assignment task was to evaluate the generated model against the evaluation set downloaded earlier containing `r nrow(eval)` unseen examples. The following answers were obtained:

```{r predict_answers}
answers <- predict(modFitRF, eval[, -49])
answers
```

```{r pml_answers, echo = FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_", i, ".txt")
    write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, col.names = FALSE)
  }
}

pml_write_files(as.character(answers))
```

# Conclusion

This assignment described the design and implementation of a learning model using random forests. It outlined the process of data cleaning and feature selection. The loaded data was then split into training and testing sets, and these were used to train and evaluate the fitted data model. Finally, the results and observations were presented.